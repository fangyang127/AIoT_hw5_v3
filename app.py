import math
import re
from typing import Dict, List, Optional, Tuple

import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


# å¿«å–åˆ†é¡æ¨¡å‹ï¼Œé¿å…é‡è¤‡è¼‰å…¥
@st.cache_resource(show_spinner=False)
def load_detector(model_name: str):
    return pipeline(
        "text-classification",
        model=model_name,
        device=-1,  # CPU
    )


# ä½¿ç”¨ distilgpt2 ä½œç‚ºè¼•é‡å›°æƒ‘åº¦æ¨¡å‹ï¼Œé¿å…è³‡æºçˆ†æ‰
@st.cache_resource(show_spinner=False)
def load_ppl_model():
    tok = AutoTokenizer.from_pretrained("distilgpt2")
    mdl = AutoModelForCausalLM.from_pretrained("distilgpt2")
    return tok, mdl


def read_uploaded_file(file) -> str:
    """å°‡ä¸Šå‚³æª”æ¡ˆå…§å®¹è§£ç¢¼æˆæ–‡å­—ï¼Œå¿½ç•¥ç„¡æ³•è§£ç¢¼çš„å­—å…ƒã€‚"""
    try:
        return file.read().decode("utf-8", errors="ignore")
    except Exception:
        return ""


def _heuristic_ai_boost(text: str) -> float:
    """
    è‹¥æ–‡æœ¬åŒ…å«å¸¸è¦‹ LLM è‡ªæˆ‘æè¿°èªå¥ï¼Œå° AI æ©Ÿç‡åšåŠ æ¬Šã€‚
    é€™äº›ç‰‡èªåœ¨çœŸå¯¦äººé¡æ–‡æœ¬ä¸­å°‘è¦‹ï¼Œå¯æå‡åµæ¸¬ç‡ã€‚
    """
    patterns = [
        r"\bas an ai language model\b",
        r"\bi do not have access to real[- ]time data\b",
        r"\bi don't have browsing capabilities\b",
        r"\bhere (is|are) (a|some) (concise|brief)\b",
        r"\bprovide (bullet points|a summary)\b",
        r"\bi cannot provide personal experiences\b",
        r"\bi am an artificial intelligence\b",
        r"\bi'm an ai\b",
        r"\bglad to help\b",
        r"\bassistant\b",
        r"\bas an assistant\b",
        r"\bi cannot fulfill that request\b",
        r"\bi don't have feelings\b",
    ]
    text_lower = text.lower()
    return 0.6 if any(re.search(p, text_lower) for p in patterns) else 0.0


def _structure_ai_boost(text: str) -> float:
    """
    è‹¥æ–‡æœ¬åŒ…å«å¤§é‡æ¢åˆ—/ä»»å‹™è¦ç¯„æˆ–æ˜é¡¯ AI ä½œæ¥­é—œéµå­—ï¼Œå¢åŠ  AI æ©Ÿç‡ã€‚
    é€™é¡æ ¼å¼åœ¨èª²ä½œéœ€æ±‚èˆ‡ AI èªªæ˜æ–‡ä»¶ä¸­å¸¸è¦‹ã€‚
    """
    lower = text.lower()
    keywords = [
        "chatgpt",
        "ai agent",
        "streamlit",
        "demo",
        "github",
        "repository",
        "éœ€é™„ä¸Š",
        "å¿…è¦",
        "é¡Œç›®",
        "ä½œæ¥­",
    ]
    bullet_markers = len(re.findall(r"^\s*[\d-]+\.", text, flags=re.MULTILINE))
    keyword_hit = any(k in lower for k in keywords)
    boost = 0.2 if keyword_hit else 0.0
    if bullet_markers >= 3:
        boost += 0.1
    return boost


def _gpt2_perplexity(text: str) -> Optional[float]:
    """è¨ˆç®— distilgpt2 å›°æƒ‘åº¦ï¼Œæ–‡æœ¬éçŸ­æ™‚è¿”å› Noneã€‚"""
    if len(text.split()) < 8:
        return None
    tok, mdl = load_ppl_model()
    enc = tok(text, return_tensors="pt", truncation=True, max_length=512)
    with torch.no_grad():
        out = mdl(**enc, labels=enc["input_ids"])
        loss = out.loss
    return math.exp(loss.item())


def predict(
    text: str,
    use_ensemble: bool = True,
    use_perplexity: bool = True,
) -> Optional[Tuple[float, float, float, Dict[str, float]]]:
    """
    å›å‚³ (ai_prob, human_prob, max_confidence, breakdown)
    - ai_prob: AI ç”Ÿæˆæ©Ÿç‡
    - human_prob: äººé¡æ’°å¯«æ©Ÿç‡
    - max_confidence: æœ€é«˜åˆ†æ•¸ï¼Œç”¨æ–¼ä½ä¿¡å¿ƒæç¤º
    - breakdown: ç´€éŒ„å„æ¨¡å‹è¼¸å‡ºï¼Œä¾¿æ–¼é™¤éŒ¯
    """
    text = (text or "").strip()
    if not text:
        return None

    model_names = ["roberta-base-openai-detector"]  # Fake / Real
    if use_ensemble:
        model_names.append("Hello-SimpleAI/chatgpt-detector-roberta")  # ChatGPT / Human

    ai_scores: List[float] = []
    human_scores: List[float] = []
    breakdown: Dict[str, float] = {}

    for name in model_names:
        clf = load_detector(name)
        outputs = clf(
            text,
            truncation=True,
            max_length=512,
            return_all_scores=True,
        )[0]
        score_map = {o["label"].lower(): float(o["score"]) for o in outputs}

        if "fake" in score_map and "real" in score_map:
            ai_scores.append(score_map["fake"])
            human_scores.append(score_map["real"])
            breakdown[f"{name}_ai"] = score_map["fake"]
            breakdown[f"{name}_human"] = score_map["real"]
        elif "chatgpt" in score_map and "human" in score_map:
            ai_scores.append(score_map["chatgpt"])
            human_scores.append(score_map["human"])
            breakdown[f"{name}_ai"] = score_map["chatgpt"]
            breakdown[f"{name}_human"] = score_map["human"]

    if not ai_scores or not human_scores:
        return None

    # ä¾å„æ¨¡å‹ç½®ä¿¡åº¦ (|ai-human|) åŠ æ¬Šå¹³å‡
    ai_prob = 0.0
    human_prob = 0.0
    weight_sum = 0.0
    for ai_val, human_val in zip(ai_scores, human_scores):
        weight = max(abs(ai_val - human_val), 0.1)
        ai_prob += ai_val * weight
        human_prob += human_val * weight
        weight_sum += weight
    ai_prob = ai_prob / weight_sum
    human_prob = human_prob / weight_sum

    # é‡å°æ˜é¡¯ LLM ç‰‡èªåšå¼·åˆ¶åå‘ AI
    heuristic = _heuristic_ai_boost(text)
    if heuristic > 0:
        ai_prob = 0.95
        human_prob = 0.05
    else:
        # ä½¿ç”¨å›°æƒ‘åº¦ä½œç‚ºè¼”åŠ©ï¼šä½å›°æƒ‘åº¦ä»£è¡¨è¼ƒåƒæ¨¡å‹ç”Ÿæˆ
        if use_perplexity:
            ppl = _gpt2_perplexity(text)
            if ppl is not None:
                if ppl < 15:
                    ai_prob += 0.25
                elif ppl < 30:
                    ai_prob += 0.15

        # æ¢åˆ—/èª²ä½œå‹æ–‡æœ¬é©åº¦å¾€ AI åç§»
        ai_prob += _structure_ai_boost(text)

    # æ­£è¦åŒ–è®“ AI% + Human% = 1
    total = ai_prob + human_prob
    if total > 0:
        ai_prob = ai_prob / total
        human_prob = human_prob / total
    else:
        human_prob = 1.0 - ai_prob

    max_confidence = max(ai_prob, human_prob)
    return ai_prob, human_prob, max_confidence, breakdown


st.set_page_config(
    page_title="AI / Human æ–‡ç« åµæ¸¬å™¨",
    page_icon="ğŸ§­",
    layout="centered",
)

st.title("ğŸ§­ AI / Human æ–‡ç« åµæ¸¬å™¨")
st.write(
    "è¼¸å…¥ä¸€æ®µæ–‡æœ¬æˆ–ä¸Šå‚³æ–‡å­—æª”ï¼Œç«‹å³ä¼°è¨ˆè©²æ®µæ–‡å­—ç‚º **AI ç”Ÿæˆ** æˆ– **äººé¡æ’°å¯«** çš„æ©Ÿç‡ã€‚"
)

# å´é‚Šè¨­å®šï¼šé¿å… Streamlit Cloud è³‡æºçˆ†æ‰
st.sidebar.header("è¨­å®š / è³‡æº")
light_mode = st.sidebar.checkbox("è¼•é‡æ¨¡å¼ï¼ˆå–®æ¨¡å‹ã€ç„¡å›°æƒ‘åº¦ï¼‰", value=True)
if light_mode:
    use_ensemble = False
    use_perplexity = False
else:
    use_ensemble = st.sidebar.checkbox("å•Ÿç”¨é›™æ¨¡å‹æŠ•ç¥¨ï¼ˆè¼ƒæº–ç¢ºï¼Œè¼ƒè€—è³‡æºï¼‰", value=True)
    use_perplexity = st.sidebar.checkbox("å•Ÿç”¨å›°æƒ‘åº¦è¼”åŠ©ï¼ˆè¼ƒè€—è³‡æºï¼‰", value=False)

st.sidebar.info("è‹¥åœ¨é›²ç«¯å‡ºç¾è³‡æºä¸è¶³ï¼Œè«‹é–‹å•Ÿã€Œè¼•é‡æ¨¡å¼ã€ã€‚")

# é è¨­æ¨£ä¾‹
sample_texts = {
    "AI ç¯„ä¾‹": (
        "Certainly! Here is a concise, well-structured overview of the requested topic. "
        "As an AI language model, I will provide bullet points, a short summary, and a "
        "polite closing statement to ensure clarity and coherence."
    ),
    "äººé¡ç¯„ä¾‹": (
        "æ˜¨å¤©åŠ ç­åˆ°åä¸€é»ï¼Œå›å®¶è·¯ä¸Šçªç„¶ä¸‹èµ·äº†å¤§é›¨ï¼Œè·¯é‚Šæ”¤çš„è±†æ¼¿é‚„æ˜¯æº«çš„ï¼Œ"
        "å–å®Œæ‰è¦ºå¾—é€™é€±æœ«ä¸€å®šè¦è£œå€‹çœ ã€‚"
    ),
    "å­¸è¡“ç¯„ä¾‹": (
        "The experiment demonstrates that introducing a lightweight regularization term "
        "improves generalization on small datasets without significantly increasing "
        "computational cost."
    ),
}

if "input_text" not in st.session_state:
    st.session_state.input_text = ""


def load_sample(name: str):
    st.session_state.input_text = sample_texts[name]


col_left, col_right = st.columns([3, 1])
with col_left:
    st.text_area(
        "è¼¸å…¥æ–‡å­—",
        key="input_text",
        height=200,
        placeholder="è²¼ä¸Šè¦åµæ¸¬çš„å…§å®¹ï¼Œæˆ–ä½¿ç”¨å³å´ç¯„ä¾‹å¿«é€Ÿæ¸¬è©¦ã€‚",
    )
with col_right:
    st.markdown("ç¯„ä¾‹æ–‡æœ¬")
    for label in sample_texts.keys():
        st.button(f"è¼‰å…¥ {label}", on_click=load_sample, args=(label,))

uploaded_file = st.file_uploader("æˆ–ä¸Šå‚³ç´”æ–‡å­—æª” (.txt)", type=["txt"])

text_from_file = ""
if uploaded_file is not None:
    text_from_file = read_uploaded_file(uploaded_file)
    if text_from_file:
        st.success("å·²è®€å–æª”æ¡ˆå…§å®¹ï¼Œå°‡å„ªå…ˆä½¿ç”¨æª”æ¡ˆæ–‡å­—é€²è¡Œåˆ¤å®šã€‚")
    else:
        st.warning("æª”æ¡ˆå…§å®¹ç„¡æ³•è§£ç¢¼ï¼Œè«‹ç¢ºèªç‚º UTF-8 ç´”æ–‡å­—ã€‚")

st.markdown("---")

if st.button("é–‹å§‹åµæ¸¬"):
    text = text_from_file or st.session_state.input_text
    if not text.strip():
        st.warning("è«‹å…ˆè¼¸å…¥æ–‡å­—æˆ–ä¸Šå‚³æª”æ¡ˆã€‚")
    else:
        with st.spinner("æ¨¡å‹æ¨è«–ä¸­ï¼Œè«‹ç¨å€™..."):
            result = predict(
                text,
                use_ensemble=use_ensemble,
                use_perplexity=use_perplexity,
            )

        if result is None:
            st.error("æœªå–å¾—æœ‰æ•ˆè¼¸å…¥ï¼Œè«‹é‡è©¦ã€‚")
        else:
            ai_prob, human_prob, max_conf, breakdown = result
            st.subheader("çµæœ")
            st.write(
                f"AI ç”Ÿæˆæ©Ÿç‡ï¼š**{ai_prob * 100:.1f}%** | äººé¡æ’°å¯«æ©Ÿç‡ï¼š**{human_prob * 100:.1f}%**"
            )

            bar_ai, bar_human = st.columns(2)
            with bar_ai:
                st.progress(ai_prob)
                st.caption("AI ç”Ÿæˆ")
            with bar_human:
                st.progress(human_prob)
                st.caption("äººé¡æ’°å¯«")

            label = "AI ç”Ÿæˆ" if ai_prob >= human_prob else "äººé¡æ’°å¯«"
            st.info(f"æ¨¡å‹åˆ¤æ–·ï¼š**{label}**")

            with st.expander("æ¨¡å‹ç´°ç¯€", expanded=False):
                for k, v in breakdown.items():
                    st.write(f"{k}: {v:.3f}")

            if max_conf < 0.6:
                st.warning("æ¨¡å‹ä¿¡å¿ƒåä½ï¼Œçµæœåƒ…ä¾›åƒè€ƒã€‚")

st.markdown("---")
st.caption(
    "éš±ç§æç¤ºï¼šæ‰€æœ‰æ¨è«–åƒ…åœ¨æœ¬åœ°ç«¯åŸ·è¡Œï¼Œä¸æœƒä¸Šå‚³æˆ–å„²å­˜æ‚¨çš„æ–‡æœ¬ã€‚è¼¸å…¥éçŸ­æ™‚ï¼Œæ¨¡å‹ä¿¡å¿ƒå¯èƒ½è¼ƒä½ã€‚"
)
